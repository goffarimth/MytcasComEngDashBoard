import asyncio
import pandas as pd
from datetime import datetime
from playwright.async_api import async_playwright

class TCASScraperNewStyle:
    def __init__(self):
        self.base_url = "https://course.mytcas.com"
        self.results = []

    async def _get_search_input(self, page):
        selectors = [
            "input[placeholder*='‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢']",
            "input[placeholder*='‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤']",
            "input[type='search']",
            "input.search-input",
            "#search-input"
        ]
        for selector in selectors:
            try:
                return await page.wait_for_selector(selector, timeout=3000)
            except:
                continue
        return None

    async def _get_result_items(self, page):
        selectors = [
            ".t-programs > li", ".program-list li", ".search-results li",
            ".results li", "[data-testid='program-item']"
        ]
        for selector in selectors:
            try:
                results = await page.query_selector_all(selector)
                if results:
                    return results
            except:
                continue
        return []

    async def _parse_program_item(self, li, keyword):
        try:
            text = await li.inner_text()
            anchor = await li.query_selector("a")
            if not anchor:
                return None
            href = await anchor.get_attribute("href")
            link = href if href.startswith("http") else f"{self.base_url}{href}"

            lines = [line.strip() for line in text.strip().splitlines() if line.strip()]
            title = lines[0] if len(lines) > 0 else ""
            faculty = lines[1] if len(lines) > 1 else ""
            university = lines[2] if len(lines) > 2 else ""

            return {
                'keyword': keyword,
                'program_name': title,
                'faculty': faculty,
                'university': university,
                'url': link,
                'raw_text': text
            }
        except:
            return None

    async def _search_programs(self, page, keyword):
        await page.goto(self.base_url, wait_until='networkidle', timeout=30000)
        await asyncio.sleep(1)

        search_box = await self._get_search_input(page)
        if not search_box:
            print("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤")
            return []

        await search_box.fill("")
        await search_box.fill(keyword)
        await search_box.press("Enter")
        await asyncio.sleep(2)

        items = await self._get_result_items(page)
        found = []
        for li in items:
            data = await self._parse_program_item(li, keyword)
            if data:
                found.append(data)
        return found

    async def _extract_details(self, page, program):
        try:
            await page.goto(program['url'], wait_until='networkidle', timeout=30000)
            await asyncio.sleep(1.5)

            def_field = lambda: "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
            info = {
                '‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô': program['keyword'],
                '‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': program['program_name'],
                '‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢': program['university'],
                '‡∏Ñ‡∏ì‡∏∞': program['faculty'],
                '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': def_field(),
                '‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢': def_field(),
                '‡∏•‡∏¥‡∏á‡∏Å‡πå': program['url'],
                '‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }

            fields = {
                '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£': ["dt:has-text('‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£') + dd", ".program-type"],
                '‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢': ["dt:has-text('‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢') + dd", ".fee-info", ".tuition-fee"]
            }

            for field, selectors in fields.items():
                for selector in selectors:
                    try:
                        el = await page.query_selector(selector)
                        if el:
                            txt = (await el.inner_text()).strip()
                            if txt:
                                info[field] = txt
                                break
                    except:
                        continue

            self.results.append(info)
            print(f"‚úÖ ‡πÄ‡∏Å‡πá‡∏ö: {info['‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£'][:40]} | üí∞ {info['‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢']}")
        except Exception as e:
            print(f"‚ö†Ô∏è ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î: {str(e)}")

    async def run(self, keywords):
        print("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• TCAS ‡πÉ‡∏´‡∏°‡πà\n" + "-" * 50)

        async with async_playwright() as pw:
            browser = await pw.chromium.launch(headless=True)
            context = await browser.new_context(locale="th-TH")
            page = await context.new_page()

            collected = []
            for kw in keywords:
                print(f"\nüîé ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô: {kw}")
                programs = await self._search_programs(page, kw)
                collected.extend(programs)
                await asyncio.sleep(1)

            for i, program in enumerate(collected):
                print(f"\n[{i+1}/{len(collected)}] ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£: {program['program_name'][:40]}")
                await self._extract_details(page, program)
                await asyncio.sleep(1)

            await browser.close()

    def export_excel(self, name="result_tcas"):
        if not self.results:
            print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")
            return

        df = pd.DataFrame(self.results)

        # üîç ‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 3 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏µ‡πâ
        before = len(df)
        df.drop_duplicates(subset=["‡∏ä‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£", "‡∏Ñ‡∏ì‡∏∞", "‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢"], inplace=True)
        after = len(df)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        fname = f"{name}_{timestamp}.xlsx"
        df.to_excel(fname, index=False, engine='openpyxl')

        print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå: {fname} | {after} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ (‡∏•‡∏ö‡∏ã‡πâ‡∏≥ {before - after} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£)")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ
        print("\nüìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥:")
        print(df["‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô"].value_counts())
        print(f"üí∞ ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢: {df[df['‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢'] != '‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•'].shape[0]}")


async def main():
    scraper = TCASScraperNewStyle()
        # üîç ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÑ‡∏ß‡πâ‡∏•‡πà‡∏ß‡∏á‡∏´‡∏ô‡πâ‡∏≤
    keywords = [
        "‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå",
        "‡∏Ñ‡∏ì‡∏∞‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå ‡∏ß‡∏¥‡∏®‡∏ß‡∏Å‡∏£‡∏£‡∏°‡∏õ‡∏±‡∏ç‡∏ç‡∏≤‡∏õ‡∏£‡∏∞‡∏î‡∏¥‡∏©‡∏ê‡πå"  # ‚Üê ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ô‡∏µ‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤
    ]

    await scraper.run(keywords)
    scraper.export_excel()

if __name__ == "__main__":
    asyncio.run(main())
